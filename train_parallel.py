#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è PPO —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Ä–µ–¥–∞–º–∏ RogueEnv.
"""

import env
import os
import torch
from torch import nn
import numpy as np

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback

from model_save_callback import SaveEveryNStepsCallback
from mean_reward_per_batch_callback import MeanRewardPerBatchCallback

SNAPSHOT_NAME = "checkpoints/ppo_rogue_parallel_98304_steps.zip"

class CustomCNNFeatureExtractor(BaseFeaturesExtractor):
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π CNN-—ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ —Ñ–æ—Ä–º—ã (C=128, H=24, W=80).
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç embedding –¥–ª—è —Å–∂–∞—Ç–∏—è one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏.
    """
    
    def __init__(self, observation_space, features_dim: int = 256):
        super().__init__(observation_space, features_dim)
        n_channels, height, width = observation_space.shape

        # Embedding —Å–ª–æ–π –¥–ª—è —Å–∂–∞—Ç–∏—è one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.embedding = nn.Sequential(
            nn.Conv2d(n_channels, 64, kernel_size=1),  # 128‚Üí64: —Å–∂–∏–º–∞–µ–º one-hot
            nn.BatchNorm2d(64),
            nn.ReLU(),
        )
        
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è CNN –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å batch normalization
        self.cnn = nn.Sequential(
            # 1-–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            
            # 2-–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫ —Å –±–æ–ª—å—à–∏–º kernel –¥–ª—è –ª—É—á—à–µ–≥–æ receptive field
            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),  # 24x80 ‚Üí 12x40
            nn.BatchNorm2d(256),
            nn.ReLU(),
            
            # 3-–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),  # 12x40 ‚Üí 6x20
            nn.BatchNorm2d(512),
            nn.ReLU(),
            
            # 4-–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è
            nn.Conv2d(512, features_dim, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(features_dim),
            nn.ReLU(),
            
            # –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å—Ä–µ–¥–Ω–∏–π –ø—É–ª
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            
            # Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
            nn.Dropout(0.2)
        )

    def forward(self, observations: torch.Tensor) -> torch.Tensor:
        # –°–Ω–∞—á–∞–ª–∞ —Å–∂–∏–º–∞–µ–º one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        x = self.embedding(observations)  # (B, 128, 24, 80) ‚Üí (B, 64, 24, 80)
        # –ó–∞—Ç–µ–º –ø—Ä–∏–º–µ–Ω—è–µ–º CNN
        return self.cnn(x)  # (B, 64, 24, 80) ‚Üí (B, features_dim)


def make_rogue_env(rank: int = 0, max_steps: int = 1024):
    """–§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å—Ä–µ–¥—ã RogueEnv —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."""
    def _init():
        # –ö–∞–∂–¥–∞—è —Å—Ä–µ–¥–∞ –±—É–¥–µ—Ç –∏–º–µ—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä RogueInterface
        rogue_env = env.RogueEnv(max_steps=max_steps)
        return rogue_env
    return _init


def train_parallel_ppo():
    """–û–±—É—á–µ–Ω–∏–µ PPO —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º–∏ —Å—Ä–µ–¥–∞–º–∏ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."""
    print("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è PPO...")
    
    # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    num_envs = 8  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    max_steps_per_env = 1024  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤
    total_timesteps = 500000# if SNAPSHOT_NAME is None else (500000 - int(SNAPSHOT_NAME.split('_')[-2]))  # –£–≤–µ–ª–∏—á–µ–Ω–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏
    os.makedirs("./checkpoints", exist_ok=True)
    os.makedirs("./logs", exist_ok=True)
    
    print(f"–°–æ–∑–¥–∞–Ω–∏–µ {num_envs} –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö —Å—Ä–µ–¥...")
    
    # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω—É—é —Å—Ä–µ–¥—É
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º DummyVecEnv –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏, –º–æ–∂–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –Ω–∞ SubprocVecEnv –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    vec_env = DummyVecEnv([
        make_rogue_env(rank=i, max_steps=max_steps_per_env) 
        for i in range(num_envs)
    ])
    
    print("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ PPO —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
    
    # –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–ª–∏—Ç–∏–∫–∏
    policy_kwargs = dict(
        net_arch=dict(
            pi=[256, 256, 128],  # –ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∞—è policy —Å–µ—Ç—å
            vf=[256, 256, 128]   # –ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∞—è value —Å–µ—Ç—å
        ),
        features_extractor_class=CustomCNNFeatureExtractor,
        features_extractor_kwargs=dict(features_dim=256),  # –£–≤–µ–ª–∏—á–µ–Ω —Ä–∞–∑–º–µ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        activation_fn=nn.ReLU,
        normalize_images=False,  # –ù–∞—à–∏ –¥–∞–Ω–Ω—ã–µ —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã
    )
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ PPO —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    if SNAPSHOT_NAME is not None:
        model = PPO.load(SNAPSHOT_NAME, env=vec_env)
        print('Model loaded from snapshot:', SNAPSHOT_NAME)
    else:
        model = PPO(
            policy="MlpPolicy",
            env=vec_env,
            policy_kwargs=policy_kwargs,
            verbose=1,
            
            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            learning_rate=1e-4,  # –ë–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π learning rate
            n_steps=512,  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–∏—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π
            batch_size=256,  # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: n_steps * num_envs / 16
            n_epochs=10,  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –±–∞—Ç—á
            
            # –í–∞–∂–Ω—ã–µ PPO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            gamma=0.995,  # –í—ã—Å–æ–∫–∏–π discount –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            gae_lambda=0.95,  # GAE –ø–∞—Ä–∞–º–µ—Ç—Ä
            clip_range=0.2,  # PPO clipping
            ent_coef=0.005,  # Entropy coefficient –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            vf_coef=0.5,  # Value function coefficient
            max_grad_norm=0.5,  # Gradient clipping
            
            device=device,
            tensorboard_log="./tb/",
            seed=42  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        )
        print('Cold start model created')
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    print("\n–í–∞–ª–∏–¥–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:")
    print(f"  n_steps: {model.n_steps}")
    print(f"  batch_size: {model.batch_size}")
    print(f"  num_envs: {num_envs}")
    print(f"  rollout_buffer_size: {model.n_steps * num_envs}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∏
    rollout_size = model.n_steps * num_envs
    assert model.batch_size <= rollout_size, f"batch_size ({model.batch_size}) > rollout_size ({rollout_size})"
    assert rollout_size % model.batch_size == 0, f"rollout_size –Ω–µ –∫—Ä–∞—Ç–µ–Ω batch_size"
    print("  ‚úÖ –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã!")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–ª–ª–±–µ–∫–æ–≤
    checkpoint_callback = CheckpointCallback(
        save_freq=max(2000, num_envs * model.n_steps),  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ ~1000 —à–∞–≥–æ–≤
        save_path="./checkpoints/",
        name_prefix="ppo_rogue_parallel"
    )

    reward_per_batch_callback = MeanRewardPerBatchCallback(verbose=1)
    
    # –û—á–∏—â–∞–µ–º —Ñ–∞–π–ª rewards.txt
    with open("rewards.txt", 'w') as f:
        f.write("")
    
    print("\n–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
    print(f"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ä–µ–¥: {num_envs}")
    print(f"  - –®–∞–≥–æ–≤ –Ω–∞ —Å—Ä–µ–¥—É –∑–∞ —ç–ø–∏–∑–æ–¥: {max_steps_per_env}")
    print(f"  - –û–±—â–∏—Ö timesteps: {total_timesteps}")
    print(f"  - N_steps (rollout): {model.n_steps}")
    print(f"  - Batch size: {model.batch_size}")
    print(f"  - Learning rate: {model.learning_rate}")
    print(f"  - Gamma: {model.gamma}")
    
    # –û–±—É—á–µ–Ω–∏–µ
    try:
        start_timesteps = 0
        if SNAPSHOT_NAME is not None:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –∏–∑ –∏–º–µ–Ω–∏ checkpoint'–∞
            import re
            match = re.search(r'(\d+)_steps', SNAPSHOT_NAME)
            if match:
                start_timesteps = int(match.group(1))
        
        remaining_timesteps = total_timesteps - start_timesteps
        if remaining_timesteps > 0:
            model.learn(
                total_timesteps=remaining_timesteps,
                callback=[checkpoint_callback, reward_per_batch_callback],
                progress_bar=True
            )
        else:
            print("–ú–æ–¥–µ–ª—å —É–∂–µ –æ–±—É—á–µ–Ω–∞ –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤")
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    model.save("ppo_rogue_parallel_final")
    print("–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'ppo_rogue_parallel_final'")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    print("\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏...")
    test_trained_model(model, vec_env, num_episodes=3)
    
    # –ó–∞–∫—Ä—ã—Ç–∏–µ —Å—Ä–µ–¥
    vec_env.close()
    print("–í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Å—Ä–µ–¥–∞ –∑–∞–∫—Ä—ã—Ç–∞")


def test_trained_model(model, vec_env, num_episodes: int = 1):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π."""
    print(f"\n{'='*50}")
    print(f"–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –û–ë–£–ß–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò")
    print(f"{'='*50}")
    
    for episode in range(num_episodes):
        print(f"\n--- –¢–µ—Å—Ç–æ–≤—ã–π —ç–ø–∏–∑–æ–¥ {episode + 1}/{num_episodes} ---")
        
        obs = vec_env.reset()
        episode_rewards = [0.0] * vec_env.num_envs
        episode_lengths = [0] * vec_env.num_envs
        active_envs = [True] * vec_env.num_envs
        
        max_test_steps = 100  # –ú–∞–∫—Å–∏–º—É–º —à–∞–≥–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞
        
        for step in range(max_test_steps):
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π
            actions, _ = model.predict(obs, deterministic=True)
            obs, rewards, dones, infos = vec_env.step(actions)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ–ª—å–∫–æ –¥–ª—è –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Ä–µ–¥
            for i in range(vec_env.num_envs):
                if active_envs[i]:
                    episode_rewards[i] += rewards[i]
                    episode_lengths[i] += 1
                    
                    # –ï—Å–ª–∏ —ç–ø–∏–∑–æ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω
                    if dones[i]:
                        print(f"  –°—Ä–µ–¥–∞ {i}: –∑–∞–≤–µ—Ä—à–µ–Ω –Ω–∞ —à–∞–≥–µ {step+1}")
                        print(f"    –ù–∞–≥—Ä–∞–¥–∞: {episode_rewards[i]:.3f}")
                        print(f"    –î–ª–∏–Ω–∞: {episode_lengths[i]}")
                        if 'player_position' in infos[i]:
                            print(f"    –ü–æ–∑–∏—Ü–∏—è –∏–≥—Ä–æ–∫–∞: {infos[i]['player_position']}")
                        if 'visited_positions' in infos[i]:
                            print(f"    –ü–æ—Å–µ—â–µ–Ω–æ –ø–æ–∑–∏—Ü–∏–π: {infos[i]['visited_positions']}")
                        active_envs[i] = False
            
            # –ö—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç –∫–∞–∂–¥—ã–µ 20 —à–∞–≥–æ–≤
            if (step + 1) % 20 == 0:
                active_count = sum(active_envs)
                avg_reward = sum(episode_rewards) / len(episode_rewards)
                print(f"  –®–∞–≥ {step+1}: –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å—Ä–µ–¥={active_count}, —Å—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞={avg_reward:.3f}")
            
            # –ï—Å–ª–∏ –≤—Å–µ —Å—Ä–µ–¥—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã
            if not any(active_envs):
                print(f"  –í—Å–µ —Å—Ä–µ–¥—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã –Ω–∞ —à–∞–≥–µ {step+1}")
                break
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–ø–∏–∑–æ–¥–∞
        print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–ø–∏–∑–æ–¥–∞ {episode + 1}:")
        print(f"  –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {sum(episode_rewards) / len(episode_rewards):.3f}")
        print(f"  –ú–∞–∫—Å –Ω–∞–≥—Ä–∞–¥–∞: {max(episode_rewards):.3f}")
        print(f"  –ú–∏–Ω –Ω–∞–≥—Ä–∞–¥–∞: {min(episode_rewards):.3f}")
        print(f"  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {sum(episode_lengths) / len(episode_lengths):.1f}")
    
    print(f"\n{'='*50}")
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print(f"{'='*50}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""
    print("üéÆ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ Rogue —Å PPO")
    print("=" * 40)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
    print("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π...")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ PyTorch –∏ CUDA
    print(f"PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
    if torch.cuda.is_available():
        print(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.get_device_name(0)}")
        print(f"CUDA –ø–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    else:
        print("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–π RAM
    try:
        import psutil
        ram_gb = psutil.virtual_memory().total / 1e9
        print(f"–î–æ—Å—Ç—É–ø–Ω–∞—è RAM: {ram_gb:.1f} GB")
        if ram_gb < 8:
            print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –º–µ–Ω–µ–µ 8GB RAM –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ")
    except ImportError:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å RAM (—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ psutil)")
    
    print("-" * 40)
    
    try:
        train_parallel_ppo()
        print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (Ctrl+C)")
        print("Checkpoint'—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ ./checkpoints/")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        print("\n–ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
        print("1. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ª–∏ –∏–≥—Ä–∞ rogue: /usr/games/rogue")
        print("2. –î–æ—Å—Ç—É–ø–µ–Ω –ª–∏ tmux")
        print("3. –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –ø–∞–º—è—Ç–∏")
        raise


if __name__ == "__main__":
    main()
